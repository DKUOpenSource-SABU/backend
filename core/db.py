import os
import pickle

import pandas as pd
from scipy.spatial import ConvexHull
import numpy as np
from pathlib import Path
import sqlite3
import json


from clustering.kmeans_module import *
from clustering.sector_visualization import sector_visualization


# ------- DB ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”© ------
# ì‘ì„±ì : ê¹€íƒœí˜•
DB_PATH = "./core/strategy.db"
def init_db():
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS max_strategy (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                total_return REAL NOT NULL,
                strategy TEXT NOT NULL
            );
        """)
        conn.commit()

# DB ì´ˆê¸°í™”
init_db()

if not os.path.exists('./data/ticker_etf.csv') or \
    not os.path.exists('./data/ticker_stock.csv'):
    raise FileNotFoundError("í•„ìš”í•œ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \
                            ./data/ í´ë”ì— ticker_etf.csvì™€ \
                            ticker_stock.csv íŒŒì¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
print("DB Initialize...")

# ETF í‹°ì»¤ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
df_etf = pd.read_csv('./data/ticker_etf.csv', encoding='utf-8')
df_etf = df_etf.iloc[:-1]
df_etf = df_etf.drop("1 yr % CHANGE", axis=1)
df_etf['SECTOR'] = "ETF"

# ì£¼ì‹ í‹°ì»¤ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
df_stock = pd.read_csv('./data/ticker_stock.csv', encoding='utf-8')
df_stock = df_stock.rename(columns={'Symbol': 'SYMBOL', 
                                    'Name': 'NAME',
                                    'Last Sale': 'LAST PRICE', 
                                    '% Change': '% CHANGE',
                                    'Sector' : 'SECTOR'})
df_stock['SECTOR'] = df_stock['SECTOR'].fillna('N/A')

# ê³µí†µ ì»¬ëŸ¼ ì¶”ì¶œ ë° ë°ì´í„° ë³‘í•© (ì£¼ì‹ & ETF)
common_cols = df_etf.columns.intersection(df_stock.columns)
df = pd.concat([df_stock[common_cols],
                df_etf[common_cols]], 
                ignore_index=True).dropna()
print("DB Create Complete...")


# ------- feature ì¶”ì¶œ ë° ë°ì´í„° ì •ìƒì¹˜/ì´ìƒì¹˜ ë¶„ë¦¬ -------
# ì‘ì„±ì : ê¹€ë™í˜

# ì¬ì‹œì‘ í•  ë•Œë§ˆë‹¤ ìƒˆë¡œ feature ì¶”ì¶œ ë° ì •ìƒì¹˜/ì´ìƒì¹˜ ë¶„ë¥˜ë¥¼ í•˜ì§€ ì•Šë„ë¡ ìºì‹œ ê²½ë¡œ ì„¤ì •
FEATURE_CACHE_PATH = Path("models/pretrained_feature.pkl")
NORMAL_OUTLIER_CACHE_PATH = Path("models/pretrained_normal_outlier.pkl")

# make_featureë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ì— feature ì¶”ì¶œ
# find_outlierë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ì— ì •ìƒì¹˜/ì´ìƒì¹˜ ë°ì´í„° ë¶„ë¦¬
# ì‘ì„±ì : ê¹€ë™í˜
def get_feature_data_db(df_symbols=None,
                            *,
                            force_refresh=False,
                            feature_cache_path=FEATURE_CACHE_PATH,
                            normal_outlier_cache_path=NORMAL_OUTLIER_CACHE_PATH):
    if df_symbols is None:
        df_symbols = df['SYMBOL'][:-1].tolist()
    
    if feature_cache_path.exists() and normal_outlier_cache_path.exists()\
        and not force_refresh:
        with feature_cache_path.open("rb") as f1,\
            normal_outlier_cache_path.open("rb") as f2:
            print(f"ğŸ“„ ìºì‹œ ë¡œë“œ: {feature_cache_path}, {normal_outlier_cache_path}")
            return pickle.load(f1), pickle.load(f2)

    # if normal_outlier_cache_path.exists() and not force_refresh:
    #     with normal_outlier_cache_path.open("rb") as f2:
    #         print(f"ğŸ“„ ìºì‹œ ë¡œë“œ: {normal_outlier_cache_path}")
    #         return pickle.load(f2)
        
    print("ğŸ§® feature ë° normal, outlier ì¬ê³„ì‚° ì¤‘ â€¦")
    # 1. ì „ì²´ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    df_list = read_csv_files_year_filter(df_symbols)
    if df_list is None:
        raise ValueError("read_csv_files_year_filter í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    start_date, end_date = find_shortest_period(df_list)
    if (start_date is None) or (end_date is None):
        raise ValueError("find_shortest_period í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    filtered_df_list = removed_stocks(df_list, end_date)
    if filtered_df_list is None:
        raise ValueError("removed_stocks í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    trimmed_list = same_period(filtered_df_list, start_date, end_date)
    if trimmed_list is None:
        raise ValueError("same_period í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # 2. íŠ¹ì„± ì¶”ì¶œ
    pretrained_feature_data = make_feature_df(trimmed_list)
    if pretrained_feature_data is None:
        raise ValueError("make_feature_df í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    feature_cache_path.parent.mkdir(parents=True, exist_ok=True)
    with feature_cache_path.open("wb") as f:
        pickle.dump(pretrained_feature_data, f)
        print(f"ğŸ’¾feature ìºì‹œ ì €ì¥ ì™„ë£Œ: {feature_cache_path}")

    # 3. ì´ìƒì¹˜ ì²˜ë¦¬
    pretrained_normal_data, pretrained_outlier_data = find_outlier(pretrained_feature_data)
    if (pretrained_normal_data is None) or (pretrained_outlier_data is None):
        raise ValueError("find_outlier í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    normal_outlier_cache_path.parent.mkdir(parents=True, exist_ok=True)
    with normal_outlier_cache_path.open("wb") as f:
        pickle.dump((pretrained_normal_data, pretrained_outlier_data), f)
        print(f"ğŸ’¾ì •ìƒì¹˜/ì´ìƒì¹˜ ìºì‹œ ì €ì¥ ì™„ë£Œ: {normal_outlier_cache_path}")

    return pretrained_feature_data, pretrained_normal_data, pretrained_outlier_data


# ------- ëª¨ë“  ì£¼ê°€ ë°ì´í„° ì‚¬ì „ íŠ¸ë ˆì´ë‹ -------
# ì‘ì„±ì : ê¹€íƒœí˜•

# ì¬ì‹œì‘ ì‹œë§ˆë‹¤ k_meansë¥¼ ìƒˆë¡œ ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ ìºì‹œ ê²½ë¡œ ì„¤ì •
CACHE_PATH = Path("models/pretrained_data.pkl")


# k_meansë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ íŠ¸ë ˆì´ë‹ ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ì´ í•¨ìˆ˜ëŠ” ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_pretrained_data_db(df_symbols=None,
                            *,
                            force_refresh=False,
                            cache_path=CACHE_PATH,
                            sector_cache_path=Path("models/pretrained_sectors.pkl")):
    if df_symbols is None:
        df_symbols = df['SYMBOL'][:-1].tolist()

    if cache_path.exists() and sector_cache_path.exists() and not force_refresh:
        with cache_path.open("rb") as f1, sector_cache_path.open("rb") as f2:
            print(f"ğŸ“„ ìºì‹œ ë¡œë“œ: {cache_path}, {sector_cache_path}")
            return pickle.load(f1), pickle.load(f2)

    print("ğŸ§® k_means ì¬ê³„ì‚° ì¤‘ â€¦")
    pretrained_data = k_means(df_symbols)
    if pretrained_data is None:
        raise ValueError("k_means í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    with cache_path.open("wb") as f:
        pickle.dump(pretrained_data, f)
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_path}")

    pretrained_sectors = sector_visualization()
    if pretrained_sectors is None:
        raise ValueError("sector_visualization í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    sector_cache_path.parent.mkdir(parents=True, exist_ok=True)
    with sector_cache_path.open("wb") as f:
        pickle.dump(pretrained_sectors, f)
        print(f"ğŸ’¾ ì„¹í„° ìºì‹œ ì €ì¥ ì™„ë£Œ: {sector_cache_path}")

    return pretrained_data, pretrained_sectors

# ì‚¬ì „ íŠ¸ë ˆì´ë‹ ë°ì´í„° ë¡œë”©
print("Pretraining...")
pretrained_feature_data, pretrained_normal_data, pretrained_outlier_data = \
    get_feature_data_db()
pretrained_data, pretrained_sectors = get_pretrained_data_db()
cluster_map = pretrained_data.set_index("ticker")["cluster"].to_dict()

# ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ì‹œê°í™”ë¥¼ ìœ„í•œ Convex Hull ìƒì„±
# Convex Hullì„ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™”
# ì‘ì„±ì : ê¹€íƒœí˜•
hull_list = []
for cluster in pretrained_data["cluster"].unique().tolist():
    cluster_points = pretrained_data[pretrained_data["cluster"] == cluster]
    if len(cluster_points) < 3:
        continue
    points = cluster_points[["PC1", "PC2"]].values
    hull = ConvexHull(points)
    ordered = np.append(hull.vertices, hull.vertices[0])
    hull_coords = [{"x": float(points[i][0]),
                    "y": float(points[i][1])} for i in ordered]
    hull_list.append({
        "cluster": int(cluster),
        "hull_coords" : hull_coords,
        })

max_strategy = list()


# ------- DB ì ‘ê·¼ Getter ------
# ëª¨ë“  í‹°ì»¤ ì •ë³´ ë° í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_all_tickers():
    global df
    return {
        row["SYMBOL"]: {
            **row,
            "CLUSTER": cluster_map.get(row["SYMBOL"], None)
        }
        for row in df.to_dict(orient="records")
    }

# ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ Convex Hull ì¢Œí‘œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_hull_list():
    global hull_list
    return hull_list

# ë°ì´í„°ì˜ featureë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€ë™í˜
def get_pretrained_feature():
    global pretrained_feature_data
    return pretrained_feature_data

# ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ì •ìƒì¹˜/ì´ìƒì¹˜ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€ë™í˜
def get_pretrained_normal_outlier():
    global pretrained_normal_data, pretrained_outlier_data
    return pretrained_normal_data, pretrained_outlier_data

# ì‚¬ì „ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_pretrained_data():
    global pretrained_data
    return pretrained_data

# ì‹¬ë³¼ì„ í†µí•´ì„œ í‹°ì»¤ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_tickers_by_symbol(symbols):
    global df
    if isinstance(symbols, str):
        symbols = [symbols]
    return {
        row["SYMBOL"]: {
            **row,
            "CLUSTER": cluster_map.get(row["SYMBOL"], None)
        }
        for row in df[df["SYMBOL"].isin(symbols)].to_dict(orient="records")
    }

# ì„¹í„° ì‹œê°í™” ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# ì‘ì„±ì : ê¹€íƒœí˜•
def get_pretrained_sectors():
    global pretrained_sectors
    return pretrained_sectors

def get_max_strategy():
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.execute("""
            SELECT total_return, strategy
            FROM max_strategy
            ORDER BY total_return DESC
            LIMIT 5
        """)
        rows = cursor.fetchall()
        return [
            {
                "total_return": r[0],
                "strategy": json.loads(r[1])  # JSON ë¬¸ìì—´ â†’ dict
            }
            for r in rows
        ]

def update_max_strategy(max_total_return, strategy):
    strategy_json = json.dumps(strategy, sort_keys=True)  # dict â†’ JSON ë¬¸ìì—´

    with sqlite3.connect(DB_PATH) as conn:
        # í˜„ì¬ ê°œìˆ˜ í™•ì¸
        cursor = conn.execute("SELECT COUNT(*) FROM max_strategy")
        count = cursor.fetchone()[0]

        if count < 5:
            conn.execute("INSERT INTO max_strategy (total_return, strategy) VALUES (?, ?)",
                         (max_total_return, strategy_json))
            conn.commit()
            return

        # ìµœì†Œ ìˆ˜ìµë¥  ê°€ì ¸ì˜¤ê¸°
        cursor = conn.execute("""
            SELECT id, total_return
            FROM max_strategy
            ORDER BY total_return ASC
            LIMIT 1
        """)
        min_id, min_return = cursor.fetchone()

        if max_total_return > min_return:
            conn.execute("DELETE FROM max_strategy WHERE id = ?", (min_id,))
            conn.execute("INSERT INTO max_strategy (total_return, strategy) VALUES (?, ?)",
                         (max_total_return, strategy_json))
            conn.commit()
